# T3chfest Madrid 2025 - Everything You Need to Know About Running LLMs Locally

Repository for the [T3chfest Madrid 2025](https://t3chfest.es/2025/programa/everything-about-running-llms-locally) about Everything You Need to Know About Running LLMs Locally.

## Abstract

As large language models (LLMs) become more accessible, running them locally unlocks exciting opportunities for developers, engineers, and privacy-focused users. Why rely on costly cloud AI services that share your data when you could deploy your own models tailored to your needs? In this session, we’ll dive into the advantages of local LLM deployment, from selecting the right open source model to optimizing performance on consumer hardware and integrating with your unique data. Let’s explore the journey to your own local stack for AI, and cover the important technical details such as model quantization, API integrations with IDE code assistants, and advanced methods like Retrieval-Augmented Generation (RAG) to connect your LLM to private data sources. Don’t miss out on the fun live demos that prove the bright future of open source AI is already here!

## Demos

* [Demo 1 - Local Model Serving with Podman AI](./demos/model-serving/)
* [Demo 2 - Retrieval Augmented Generation with AnythingLLM](./demos/rag/)
* [Demo 3 - Private Code Assistant with Continue](./demos/code-assistant)
* [Demo 4 - Agentic App powered by Streamlit, LangGraph and Podman AI](./demos/agentic-app)

## Slides

* [Slides Link - red.ht/local-llm](https://red.ht/local-llm)
* [Slides (PDF) - Everything You Need to Know About Running LLMs Locally](./slides/[Public]%20Everything%20You%20Need%20to%20Know%20About%20Running%20LLMs%20Locally.pdf)

## Contributors / Maintainers

* [Cedric Clyburn](github.com/cedricclyburn)
* [Roberto Carratalá](github.com/rcarrata)
